{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac19e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from collections import Counter\n",
    "sn.set()\n",
    "#from seaborn import heatmap\n",
    "#import geopandas as gpd\n",
    "#import geoplot as gplty\n",
    "#import tqdm as tqdm\n",
    "#import random\n",
    "import cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13881b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of train and test files\n",
    "train_loc = '../data/raw/train_ver2.csv'\n",
    "test_loc = '../data/raw/test_ver2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f02cacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw/train_ver2.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e40aa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw/test_ver2.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12f1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fecha_dato  ncodpers ind_empleado pais_residencia sexo  age  fecha_alta  \\\n",
      "0  2015-01-28   1375586            N              ES    H   35  2015-01-12   \n",
      "1  2015-01-28   1050611            N              ES    V   23  2012-08-10   \n",
      "2  2015-01-28   1050612            N              ES    V   23  2012-08-10   \n",
      "3  2015-01-28   1050613            N              ES    H   22  2012-08-10   \n",
      "4  2015-01-28   1050614            N              ES    V   23  2012-08-10   \n",
      "\n",
      "   ind_nuevo antiguedad  indrel  ... ind_hip_fin_ult1 ind_plan_fin_ult1  \\\n",
      "0        0.0          6     1.0  ...                0                 0   \n",
      "1        0.0         35     1.0  ...                0                 0   \n",
      "2        0.0         35     1.0  ...                0                 0   \n",
      "3        0.0         35     1.0  ...                0                 0   \n",
      "4        0.0         35     1.0  ...                0                 0   \n",
      "\n",
      "  ind_pres_fin_ult1 ind_reca_fin_ult1 ind_tjcr_fin_ult1 ind_valo_fin_ult1  \\\n",
      "0                 0                 0                 0                 0   \n",
      "1                 0                 0                 0                 0   \n",
      "2                 0                 0                 0                 0   \n",
      "3                 0                 0                 0                 0   \n",
      "4                 0                 0                 0                 0   \n",
      "\n",
      "  ind_viv_fin_ult1 ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  \n",
      "0                0             0.0                0.0                0  \n",
      "1                0             0.0                0.0                0  \n",
      "2                0             0.0                0.0                0  \n",
      "3                0             0.0                0.0                0  \n",
      "4                0             0.0                0.0                0  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Columns in the train data are \n",
      "\n",
      "fecha_dato       tiprel_1mes            ind_ahor_fin_ult1  ind_ecue_fin_ult1\n",
      "ncodpers         indresi                ind_aval_fin_ult1  ind_fond_fin_ult1\n",
      "ind_empleado     indext                 ind_cco_fin_ult1   ind_hip_fin_ult1 \n",
      "pais_residencia  conyuemp               ind_cder_fin_ult1  ind_plan_fin_ult1\n",
      "sexo             canal_entrada          ind_cno_fin_ult1   ind_pres_fin_ult1\n",
      "age              indfall                ind_ctju_fin_ult1  ind_reca_fin_ult1\n",
      "fecha_alta       tipodom                ind_ctma_fin_ult1  ind_tjcr_fin_ult1\n",
      "ind_nuevo        cod_prov               ind_ctop_fin_ult1  ind_valo_fin_ult1\n",
      "antiguedad       nomprov                ind_ctpp_fin_ult1  ind_viv_fin_ult1 \n",
      "indrel           ind_actividad_cliente  ind_deco_fin_ult1  ind_nomina_ult1  \n",
      "ult_fec_cli_1t   renta                  ind_deme_fin_ult1  ind_nom_pens_ult1\n",
      "indrel_1mes      segmento               ind_dela_fin_ult1  ind_recibo_ult1  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of data points in train: 13647309\n"
     ]
    }
   ],
   "source": [
    "#We are loading the data as a dask dataframe\n",
    "data = dd.read_csv(train_loc, dtype={'age': object,\n",
    "                        'antiguedad': object,\n",
    "                        'cod_prov': float,\n",
    "                        'ind_actividad_cliente': float,\n",
    "                        'ind_nom_pens_ult1': float,\n",
    "                        'ind_nomina_ult1': float,\n",
    "                        'ind_nuevo': float,\n",
    "                        'indrel': float,\n",
    "                        'tipodom': float,\n",
    "                        'ult_fec_cli_1t': object,\n",
    "                        'conyuemp': object,\n",
    "                        'indrel_1mes': object })\n",
    "\n",
    "print(data.head())\n",
    "print('-'*100)\n",
    "print('Columns in the train data are \\n')\n",
    "columns = list(data.columns)\n",
    "cmd.Cmd().columnize(columns, displaywidth=80)\n",
    "print('-'*100)\n",
    "customers = data.ncodpers.unique()\n",
    "print('Number of data points in train:', len(data))\n",
    "print('Number of Unique Customer in train set:', len(customers.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed44574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d62124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets change the feature names from spainish to english based on the meaning\n",
    "cols = ['fetch_date', 'cust_code', 'emp_index', 'country', 'sex', 'age', 'cust_date', 'new_cust', 'cust_seniority',\n",
    "'indrel', 'last_date_as_primary', 'cust_type', 'cust_rel', 'residence_index', 'foreigner_index', 'spouse_index',\n",
    "'joining_channel', 'deceased', 'address_type', 'prov_code','prov_name', 'activity_index', 'income', 'segmentation', \n",
    "'savings_account', 'guarentees', 'current_account', 'derivative_account', 'payroll_account', 'junior_account', 'mas_account',\n",
    "'perticular_account', 'perticular_plus', 'st_deposit', 'mt_deposits', 'lt_deposits', 'e_account', 'funds', 'mortgage',\n",
    " 'pension', 'loan', 'tax', 'credit_card', 'securities', 'home_account', 'payroll', 'pension2', 'direct_debit'\n",
    "]\n",
    "data.columns = cols\n",
    "\n",
    "print('New column names \\n')\n",
    "cmd.Cmd().columnize(cols, displaywidth=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eababc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_missing_values(data, null_columns, target_columns):\n",
    "\n",
    "  for column in null_columns:\n",
    "    if column not in target_columns:\n",
    "      print('preprocessing', column)\n",
    "      custs = data.cust_code[data[column].isna()].unique().compute()\n",
    "      print('Number of customers whose {} data is missing is {}'.format(column, custs.shape[0]))\n",
    "      available_custs = data.cust_code[(~data[column].isna()) & (data.cust_code.isin(custs))].unique().compute()\n",
    "      print('Number of customers whose data is present elsewhere is {}'.format(available_custs.shape[0]))\n",
    "\n",
    "      #Creating a dummy data set as we can not assign values to dask dataframes\n",
    "      dummy = data[data.cust_code.isin(available_custs)].compute()\n",
    "      data = data[~data.cust_code.isin(available_custs)]\n",
    "\n",
    "      available_values = dummy[(dummy.cust_code.isin(available_custs))&(~dummy[column].isna())].groupby(by='cust_code')[column].first()\n",
    "      available_dict = dict(zip(available_values.index, available_values.values))\n",
    "      dummy[column][(dummy.cust_code.isin(available_custs))&(dummy[column].isna())] = dummy['cust_code'][(dummy.cust_code.isin(available_custs))&(dummy[column].isna())].map(available_dict)\n",
    "      print('processed', column)\n",
    "      data = dd.concat([data, dd.from_pandas(dummy, npartitions = 20)])\n",
    "      print('-'*100)\n",
    "    else:\n",
    "      print('preprocessing', column)\n",
    "      print('filling na values with 0')\n",
    "\n",
    "      dummy = data[data[column].isna()].compute()\n",
    "      dummy[column] = 0\n",
    "      data = data[~data[column].isna()]\n",
    "      data = dd.concat([data, dd.from_pandas(dummy, npartitions = 20)])\n",
    "      print('processed', column)\n",
    "      print('-'*100)\n",
    "  \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6446bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts\n",
    "target_columns = ['savings_account', 'guarentees', 'current_account', 'derivative_account', 'payroll_account', 'junior_account', 'mas_account',\n",
    "'perticular_account', 'perticular_plus', 'st_deposit', 'mt_deposits', 'lt_deposits', 'e_account', 'funds', 'mortgage',\n",
    " 'pension', 'loan', 'tax', 'credit_card', 'securities', 'home_account', 'payroll', 'pension2', 'direct_debit']\n",
    "\n",
    "user_features = ['fetch_date', 'cust_code', 'emp_index', 'country', 'sex', 'age', 'cust_date', 'new_cust', 'cust_seniority',\n",
    "'indrel', 'last_date_as_primary', 'cust_type', 'cust_rel', 'residence_index', 'foreigner_index', 'spouse_index',\n",
    "'joining_channel', 'deceased', 'address_type', 'prov_code','prov_name', 'activity_index', 'income', 'segmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c677ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167132a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "null_columns = data.isna().sum()[data.isna().sum()>0].index.compute()\n",
    "data = preprocess_missing_values(data, null_columns, target_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6060957",
   "metadata": {},
   "source": [
    "If a product is bought at once it is carried and mentioned in all the subsequent months, Lets change this format, Lets create a new dataframe where we keep it mentioned in only in the month its bought.\n",
    "\n",
    "We will remove jan 2015 data as it carries all the purchases from the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dataframe for a previous date\n",
    "dummy = pd.DataFrame(\n",
    "    {\n",
    "        'cust_code': data.cust_code.unique().compute(),\n",
    "        'fetch_date': '2014-12-28'\n",
    "    }\n",
    ")\n",
    "\n",
    "new_purchases = pd.concat([data[['cust_code', 'fetch_date']+target_columns].compute(), dummy])\n",
    "\n",
    "new_purchases = new_purchases.fillna(0)\n",
    "new_purchases[target_columns] = new_purchases[target_columns].astype('uint8')\n",
    "new_purchases = new_purchases.sort_values(['cust_code', 'fetch_date'])\n",
    "\n",
    "vals = np.array(new_purchases[target_columns].values, dtype='int8')\n",
    "vals[1:] = vals[1:] - vals[:-1]\n",
    "\n",
    "new_purchases[target_columns] = vals\n",
    "#Removing the data of '2014-12-28' and '2015-01-28'\n",
    "new_purchases = new_purchases[~new_purchases.fetch_date.isin(['2014-12-28','2015-01-28'])]\n",
    "\n",
    "#Some of the products were discontinued so purchase value there becomes less than 0,\n",
    "#as we are only interested in purchases we can remove them\n",
    "for col in target_columns:\n",
    "  new_purchases[col][new_purchases[col] < 0] = 0\n",
    "\n",
    "#drop all the rows where no new purchase is made\n",
    "new_purchases = new_purchases[(new_purchases[target_columns].sum(axis=1) > 0)]\n",
    "\n",
    "new_purchases = data[user_features].merge(dd.from_pandas(new_purchases, npartitions=3), on = ['fetch_date', 'cust_code'], how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eee401",
   "metadata": {},
   "source": [
    "Some of the features are of the users which do not change with each purchase, We can make a dataset of those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = new_purchases.drop_duplicates(subset=['cust_code'], keep='last')[['cust_code', 'emp_index', 'country', 'sex', 'age', 'cust_date', 'new_cust', 'cust_seniority',\n",
    "                  'indrel', 'cust_type', 'cust_rel', 'residence_index', 'foreigner_index', 'spouse_index',\n",
    "                  'joining_channel', 'deceased', 'address_type', 'prov_code','prov_name', 'activity_index',\n",
    "                  'income', 'segmentation']]\n",
    "\n",
    "user_purchases = new_purchases[['cust_code']+target_columns].groupby(by='cust_code').sum()\n",
    "\n",
    "user_features = user_data.merge(user_purchases, on='cust_code', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a084a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_purchases = pd.read_csv('/content/drive/MyDrive/Santander/new_purchases.csv')\n",
    "user_features = pd.read_csv('/content/drive/MyDrive/Santander/user_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_purchases.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f98373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(series, plt_title, figsize, bins=14):\n",
    "  plt.figure(figsize = figsize)\n",
    "  plt.title(plt_title)\n",
    "  series = np.array(series, dtype=int)\n",
    "  sn.distplot(series, hist=True, bins=bins, kde=True, kde_kws={'bw':.2})\n",
    "  plt.axvline(series.mean(),color='midnightblue',label='Mean')    \n",
    "  plt.axvline(np.median(series),color='blue',label='Median')\n",
    "  plt.axvline(series.max(),color='indigo',label='Max')\n",
    "  plt.axvline(series.min(),color='crimson',label='Min')\n",
    "  plt.axvline(np.quantile(series, 0.25),color='red',label='First quartile - 25%')\n",
    "  plt.axvline(np.quantile(series, 0.75),color='orangered',label='Third quartile - 75%')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def stripplot(data, title, figsize):\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.title(title)\n",
    "  y = data.columns[0]\n",
    "  x = data.columns[1]\n",
    "  sn.stripplot(x=x, y=y, data=data, jitter=False, dodge=True)\n",
    "  sn.boxplot(x=x, y=y, data=data)\n",
    "  plt.xticks(rotation = 90)\n",
    "  plt.show()\n",
    "\n",
    "def print_count(series, title, show=True):\n",
    "  counts = Counter(series)\n",
    "  countlist = []\n",
    "  for c in counts:\n",
    "    countlist.append(str(c) + ':' +str(counts[c]))\n",
    "  if show:\n",
    "    print('\\n'+title)\n",
    "    cmd.Cmd().columnize(countlist, displaywidth=80)\n",
    "\n",
    "  return counts\n",
    "\n",
    "def piecountplot(series, title):\n",
    "  plt.title(title)\n",
    "  series.value_counts().plot(kind='pie', autopct='%1.2f%%')\n",
    "  plt.legend()\n",
    "\n",
    "def piepurchaseplot(dictionary, title):\n",
    "  plt.title(title)\n",
    "  plt.pie(dictionary.values(), labels = dictionary.keys(), autopct='%1.2f%%')\n",
    "  plt.legend()\n",
    "\n",
    "def countplot(series, title, figsize):\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.title(title)\n",
    "  sn.countplot(series)\n",
    "  plt.show()\n",
    "\n",
    "def barplot(series, title, figsize):\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.title(title)\n",
    "  plt.bar(series.index, series.values, color=\"royalblue\")\n",
    "  plt.show()\n",
    "\n",
    "def allplots(data, title, kind='countplot', figsize=(10,2.5)):\n",
    "  if kind == 'countplot':\n",
    "    countplot(data, title, figsize)\n",
    "  elif kind == 'piecountplot':\n",
    "    piecountplot(data, title)\n",
    "  elif kind == 'dist_plot':\n",
    "    plot_distribution(data, title, figsize)\n",
    "  elif kind == 'barplot':\n",
    "    barplot(data, title, figsize)\n",
    "  elif kind == 'piepurchaseplot':\n",
    "    piepurchaseplot(data, title)\n",
    "  elif kind == 'stripplot':\n",
    "    stripplot(data, title, figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16709b",
   "metadata": {},
   "source": [
    "Analyzing fetch_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80060285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting number of purchases in each month\n",
    "dummy = new_purchases[['fetch_date']+target_columns].groupby('fetch_date').sum()\n",
    "dummy.plot(kind='bar',stacked=True, colormap='rainbow',figsize=(20,5))\n",
    "plt.legend(loc='center left', title='Legend Title', bbox_to_anchor=(1, .4))\n",
    "plt.title('Number of purchases in each month')\n",
    "plt.show()\n",
    "print('-'*150)\n",
    "#Plotting the distribution of products in each month\n",
    "dummy = new_purchases[['fetch_date']+target_columns].groupby('fetch_date').sum()\n",
    "dummy = dummy/dummy.sum()\n",
    "dummy = dummy.T\n",
    "dummy.plot(kind='bar',stacked=True, colormap='rainbow',figsize=(20,5))\n",
    "plt.legend(loc='center left', title='Legend Title', bbox_to_anchor=(1, .4))\n",
    "plt.title('Distribution of products in each month')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a33e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
